{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow TensorBoard计算图可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择环境：Anaconda Python 3.6.6  \n",
    "安装Tensorflow：Python 3.6环境下运行pip install --upgrade --ignore-installed tensorflow  \n",
    "参考书籍：《TensorFlow实战Google深度学习框架（第2版）》  \n",
    "ipynb格式：点击阅读原文github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 TensorBoard简介\n",
    "\n",
    "TensorBoard可以有效地展示TensorFlow在运行过程中的计算图、各种指标随着时间的变化趋势以及训练中使用到的图像等信息。TensorBoard和TensorFlow程序跑在不同的进程中，TensorBoard会自动读取最新的TensorFlow日志文件，并呈现当前TensorFlow程序运行的最新状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T13:34:54.541224Z",
     "start_time": "2018-12-27T13:34:54.536211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\74575\\Desktop\\pythondata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/74575/Desktop/pythondata')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T13:35:25.141501Z",
     "start_time": "2018-12-27T13:35:18.954684Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义一个简单的计算图，实现向量加法的操作\n",
    "input1 = tf.constant([1.0, 2.0, 3.0], name='input1')\n",
    "input2 = tf.Variable(tf.random_uniform([3]), name='input2')\n",
    "output = tf.add_n([input1, input2], name='add')\n",
    "\n",
    "# 生成一个写日志的Writer，并将当前的TensorFlow计算图写入日志\n",
    "# TensorFlow提供了很多种写日志文件的API，11.3节会详细介绍\n",
    "writer = tf.summary.FileWriter(\"log\", tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上程序输出了TensorFlow计算图的信息。  \n",
    "TensorFlow安装完成时，TensorBoard会自动安装。运行以下命令便可以启动TensorBoard:\n",
    "\n",
    "`\n",
    "tensorboard --logdir=/path/to/log\n",
    "`\n",
    "\n",
    "运行以上命令会启动一个服务，这个服务的端口默认为**6006**。通过浏览器打开localhost:6006，在界面的上方，展示的内容是“GRAPHS”，表示图中可视化的内容是TensorFlow的计算图。打开TensorBoard界面会默认进入GRAPHS界面，在该界面中可以看到上面程序TensorFlow计算图的可视化结果。\n",
    "\n",
    "上方有一个“INACTIVE＂选项，点开这个选项可以看到TensorBoard能够可视化的其他内容。“INACTIVE”选项中列出的是当前没有可视化数据的项目。除了可视化TensorFlow计算图之外， TensorBoard还提供了SCALARS、IMAGES、AUDIO、DISTRIBUTIONS、HISTOGRAMS、PROJECTOR、TEXT和PROFILE项目。TensorBoard中每一栏都对应了一类信息的可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-27T12:08:17.245Z"
    }
   },
   "outputs": [],
   "source": [
    "# 在log的母文件夹，打开cmd，输入activate py36，运行该命令\n",
    "tensorboard --logdir=log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 TensorBoard计算图可视化\n",
    "\n",
    "当神经网络模型的结构更加复杂、运算更多时，其所对应的TensorFlow计算图会复杂很多。为了更好地组织可视化效果图中的计算节点，TensorBoard支持通过TensorFlow命名空间来整理可视化效果图上的节点。在TensorBoard的默认视图中，TensorFlow计算图中同一个命名空间下的所有节点会被缩略成一个节点，只有顶层命名空间中的节点才会被显示在TensorBoard可视化效果图上。\n",
    "\n",
    "前面已经介绍过变量的命名空间，以及如何通过`tf.variable_scope`函数管理变量的命名空间。除此之外，`tf.name_scope`函数也提供了命名空间管理的功能。这两个函数在大部分情况下是等价的，唯一的区别是在使用`tf.get_variable`函数时。以下代码简单地说明了这两个函数的区别:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:02:36.338451Z",
     "start_time": "2018-12-27T14:02:36.283302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/bar:0\n",
      "bar/bar:0\n",
      "a/Variable:0\n",
      "b:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. 不同的命名空间\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    # 在命名空间foo下获取变量bar，于是得到的变量名为:foo/bar\n",
    "    a = tf.get_variable(\"bar\", [1])\n",
    "    print(a.name)                                   # 输出：foo/bar:0\n",
    "\n",
    "with tf.variable_scope(\"bar\"):\n",
    "    # 在命名空间bar下获取变量bar，于是得到的变量名为:bar/bar\n",
    "    # 此时变量bar/bar和变量foo/bar并不冲突，于是可以正常运行\n",
    "    b = tf.get_variable(\"bar\", [1])\n",
    "    print(b.name)                                  # 输出：bar/bar:0\n",
    "    \n",
    "# 2. tf.Variable和tf.get_variable的区别\n",
    "with tf.name_scope(\"a\"):\n",
    "    # 使用tf.Variable函数生成变量会受tf.name_scope影响\n",
    "    a = tf.Variable([1])\n",
    "    print(a.name)                                 # 输出：a/Variable:0\n",
    "    \n",
    "    # 使用tf.get_variable函数不受tf.name_scope影响\n",
    "    a = tf.get_variable(\"b\", [1])\n",
    "    print(a.name)                                 # 输出：b:0\n",
    "    \n",
    "# 由于tf.get_varibale不受name_scope影响，所以这里会报声明重复错误\n",
    "# with tf.name_scope(\"b\"):\n",
    "#     tf.get_variable(\"b\", [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:03:16.353850Z",
     "start_time": "2018-12-27T14:03:16.348847Z"
    }
   },
   "source": [
    "通过以下代码，可以改进向量相加的样例代码，使得可视化得到的效果图更加清晰："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:06:16.458277Z",
     "start_time": "2018-12-27T14:06:16.394828Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 将输入定义放进各自的命名空间，从而使得TensorBoard可以根据\n",
    "# 命名空间来整理可视化效果图上的节点\n",
    "with tf.name_scope(\"input1\"):\n",
    "    input1 = tf.constant([1.0, 2.0, 3.0], name=\"input2\")\n",
    "with tf.name_scope(\"input2\"):\n",
    "    input2 = tf.Variable(tf.random_uniform([3]), name=\"input2\")\n",
    "output = tf.add_n([input1, input2], name=\"add\")\n",
    "\n",
    "writer = tf.summary.FileWriter(\"log\", tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出用于初始化的节点已经被缩略起来了，需要查看input2节点中具体包含了哪些运算时，可以将鼠标移动到input2节点，并点开右上角的加号。\n",
    "\n",
    "下面将给出一个样例程序来展示如何很好地可视化一个真实的神经网络结构图。继续采用5.5节中给出的架构，以下代码给出了改造后的mnist_train.py程序:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:14:24.268384Z",
     "start_time": "2018-12-27T14:14:13.781789Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import mnist_inference\n",
    "\n",
    "# 1. 定义神经网络的参数\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 3000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "\n",
    "# 2. 定义训练的过程并保存TensorBoard的log文件\n",
    "def train(mnist):\n",
    "    #  输入数据的命名空间。\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name='y-input')\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = mnist_inference.inference(x, regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 处理滑动平均的命名空间。\n",
    "    with tf.name_scope(\"moving_average\"):\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "   \n",
    "    # 计算损失函数的命名空间。\n",
    "    with tf.name_scope(\"loss_function\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    # 定义学习率、优化方法及每一轮执行训练的操作的命名空间。\n",
    "    with tf.name_scope(\"train_step\"):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            mnist.train.num_examples / BATCH_SIZE,\n",
    "            LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "        with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "    \n",
    "    # 训练模型。\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "    \n",
    "    # 将当前计算图中输出到TensorBoard日志文件\n",
    "    writer = tf.summary.FileWriter(\"log/mnist.log\", tf.get_default_graph())\n",
    "    writer.close()\n",
    "    \n",
    "    \n",
    "# 3. 主函数\n",
    "def main(argv=None): \n",
    "    mnist = input_data.read_data_sets(\"../../datasets/MNIST_data\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*上面一个cell中程序在jupyter中会一直报错，可选择使用.py来运行（文件见同目录mnist.py）。*\n",
    "\n",
    "以上程序最大的改变就是将完成类似功能的计算放到了由`tf.name_scope`函数生成的上下文管理器中。这样TensorBoard可以将这些节点有效地合并，从而突出神经网络的整体结构。\n",
    "\n",
    "节点之间有两种不同的边：一种边是通过实线表示的，这种边刻画了数据传输，边上箭头方向表达了数据传输的方向。效果图上边的粗细表示的是两个节点之间传输的标量维度的总大小，而不是传输的标量个数。另外一种边是通过虚线表示的，表达了计算之间的依赖关系。\n",
    "\n",
    "除了手动的通过TensorFlow中的命名空间来调整TensorBoard的可视化效果图，TensorBoard也会智能地调整可视化效果图上的节点。TensorBoard将TensorFlow计算图分成了主图（Main Graph）和辅助图（Auxiliary nodes）。除了自动的方式，TensorBoard也支持手工的方式来调整可视化结果。右键单击可视化效果图上的节点会弹出一个选项，这个选项可以将节点加入主图或者从主图中删除。左键选择一个节点并点击信息框下部的选项也可以完成类似的功能。**注意TensorBoard不会保存用户对计算图可视化结果的手工修改，页面刷新之后计算图可视化结果又会回到最初的样子。**\n",
    "\n",
    "除了展示TensorFlow计算图的结构，TensorBoard还可以**展示TensorFlow计算图上每个节点的基本信息以及运行时消耗的时间和空间。**可以非常直观地展现所有TensorFlow计算节点在某一次运行时所消耗的时间和内存。将以下代码加入修改后的mnist_train.py神经网络训练部分，就可以将不同迭代轮数的每个TensorFlow计算节点的运行时间和消耗的内存写入TensorBoard的日志文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(TRAINING_STEPS):\n",
    "        xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "        if i % 1000 == 0:\n",
    "            # 配置运行时需要记录的信息\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            # 运行时记录运行信息的proto\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            # 将配置信息和记录运行的proto传入运行的过程，从而记录运行时每一个节点的时间、内存信息\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step],\n",
    "                                            feed_dict={x: xs, y_:ys},\n",
    "                                            options=run_options,\n",
    "                                            run_metadata=run_metadata)\n",
    "            # 将节点在运行时的信息写入日志文件\n",
    "            train_writer.add_run_metadata(run_metadata, 'step%03d'%i)\n",
    "            print(\"After %d training step(s), loss on training batch is %g\" % (step, loss_value))\n",
    "    else:\n",
    "        _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_:ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次运行文件（见同目录mnist_info.py），并使用这个程序输出的日志启动TensorBoard，这样就可以可视化每个TensorFlow计算节点在某一次运行时所消耗的时间和空间。\n",
    "\n",
    "在Color栏中：\n",
    "- **Compute time**。可以看到在这次运行中每个TensorFlow计算节点的运行时间；\n",
    "- **Memory**。可以看到这次运行中每个TensorFlow计算节点所消耗的内存；\n",
    "- **Structure**。前面图中展示的可视化效果图都是使用默认的Structure选项。在这个视图中，灰色的节点表示没有其他节点和它拥有相同结构。如果有两个节点的结构相同，那么它们会被涂上相同的颜色；\n",
    "- **Device**。这个选项可以根据TensorFlow计算节点运行的机器给可视化效果图上的节点染色。在使用GPU时，可以通过这种方式直观地看到哪些计算节点被放到了GPU上。\n",
    "\n",
    "信息卡片：当点击TensorBoard可视化效果图中的节点时，界面的右上角会弹出一个信息卡片显示这个节点的基本信息。当点击节点为一个命名空间时，TensorBoard展示的信息卡片有这个命名空间内所有计算节点的输入、输出以及依赖关系。虽然属性（attributes)也会展示在卡片中，但是在代表命名空间的属性下不会有任何内容。当Session runs选择了某一次运行时，节点的信息卡片上也会出现这个节点运行时所消耗的时间和内存等信息。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "290.417px",
    "left": "757.273px",
    "right": "20px",
    "top": "120px",
    "width": "357.784px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
