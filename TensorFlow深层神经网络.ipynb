{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow深层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择环境：Anaconda Python 3.5.2  \n",
    "安装Tensorflow：Python 3.5环境下运行pip install --upgrade --ignore-installed tensorflow  \n",
    "参考书籍：《TensorFlow实战Google深度学习框架（第2版）》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 深度学习与深层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维基百科对深度学习的精确定义：“一类通过多层非线性变换对高复杂性数据建模算法的合集”。  \n",
    "深度学习两个非常重要的特性——多层、非线性  \n",
    "只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别，且都是线性模型。可通过TensorFlow Playground实验验证。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow提供了7种不同的非线性激活函数，比较常用的有：\n",
    "\n",
    "    tf.nn.relu\n",
    "    tf.sigmoid\n",
    "    tf.tanh\n",
    "\n",
    "使用了激活函数和偏置项的神经网络前向传播算法：\n",
    "\n",
    "    a = tf.nn.relu(tf.matmul(x,w1) + biases1)\n",
    "    y = tf.nn.relu(tf.matmul(a,w2) + biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深层神经网络实际上有组合特征提取的功能，这个特性对于解决不易提取特征向量的问题（比如图片识别、语音识别等）有很大的帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 损失函数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过神经网络解决多分类问题最常用的方法是设置n个输出节点，n为类别的个数。  \n",
    "交叉熵（cross entropy）是判断一个输出向量和期望的向量的接近程度的常用方法之一，刻画的是两个概率分布之间的距离，然而神经网络的输出却不一定是一个概率分布。Softmax回归可以将神经网络前向传播得到的结果变成概率分布。Softmax回归本身可以作为一个学习算法来优化分类结果，但在TensorFlow中softmax回归的参数被去掉了，它只是一层额外的处理层，将神经网络的输出变成一个概率分布。   \n",
    "从交叉熵公式可以看出H(p,q)≠H(q,p)。p代表正确答案，q代表预测值，H(p,q)刻画通过概率分布q来表达概率分布p的困难程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现交叉熵\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "#y_代表正确结果，y代表预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.5 2.5 3. ]\n",
      " [4.  4.5 4.5]]\n"
     ]
    }
   ],
   "source": [
    "# tf.clip_by_value函数可以将一个张量中的数值限制在一个范围之内，以避免运算错误（如log0）\n",
    "import tensorflow as tf\n",
    "v = tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "with tf.Session() as sess:\n",
    "    print(tf.clip_by_value(v,2.5,4.5).eval())\n",
    "    #v中小于2.5用2.5来替代，大于4.5用4.5来替代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.6931472 1.0986123]\n"
     ]
    }
   ],
   "source": [
    "# tf.log函数对张量中所有元素依次求对数\n",
    "v = tf.constant([1.0,2.0,3.0])\n",
    "with tf.Session() as sess:\n",
    "    print(tf.log(v).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 12]\n",
      " [21 32]]\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "# *不是矩阵乘法，而是对应元素相乘。矩阵乘法用tf.matmul\n",
    "v1 = tf.constant([[1,2],[3,4]])\n",
    "v2 = tf.constant([[5,6],[7,8]])\n",
    "with tf.Session() as sess:\n",
    "    print((v1*v2).eval())\n",
    "    print(tf.matmul(v1,v2).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据交叉熵的公式，应该将每行中的m个结果相加得到所有样例的交叉熵，再对这n行平均得到一个batch的平均交叉熵。但因为分类问题的类别数量是不变的，所以可以直接对整个矩阵做平均而不改变计算结果的意义。这样的方式可以使整个程序更加简洁。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "# tf.reduce_mean函数\n",
    "v=tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "with tf.Session() as sess:\n",
    "    print(tf.reduce_mean(v).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵一般会和softmax一起使用，所以tf对这两个功能进行了统一封装，提供了下面函数：\n",
    "\n",
    "    tf.nn.softmax_cross_entropy_with_logits() \n",
    "\n",
    "在只有一个正确答案的分类问题中，tf提供了下面函数进一步加速计算过程：\n",
    "\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现使用softmax回归之后的交叉熵损失函数\n",
    "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(y,y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于回归问题，最常用的损失函数是均方误差(MSE)。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 均方误差\n",
    "mse = tf.reduce_mean(tf.square(y_-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF不仅支持经典的损失函数，还可以优化任意的自定义损失函数。以预测商品销量问题为例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义损失函数\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(v1,v2),\n",
    "                              (v1-v2)*a,(v2-v1)*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.greater(A,B)的输入是两个张量，比较这两个输入张量中每一个元素的大小，并返回比较结果。  \n",
    "tf.where(a,b,c)函数中有三个参数，第一个为选择条件依据，True时tf.where函数会选择第二个参数中的值，False使用第三个参数中的值。注意：判断和选择都是在元素级别进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True]\n",
      "[4 3 3 4]\n"
     ]
    }
   ],
   "source": [
    "# tf.where函数和tf.greater函数的用法\n",
    "v1=tf.constant([1,2,3,4])\n",
    "v2=tf.constant([4,3,2,1])\n",
    "with tf.Session() as sess:\n",
    "    print(tf.greater(v1,v2).eval())\n",
    "    print(tf.where(tf.greater(v1,v2),v1,v2).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下列程序中，实现了一个拥有两个输入节点，一个输出节点，没有隐藏层的神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.019347 ]\n",
      " [1.0428089]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "#两个输入节点\n",
    "x=tf.placeholder(tf.float32,shape=(None,2),name=\"x-input\")\n",
    "#回归问题一般只有一个输出节点\n",
    "y_=tf.placeholder(tf.float32,shape=(None,1),name=\"y-input\")\n",
    "\n",
    "#定义一个单层的神经网络前向传播的过程，这里是简单加权和\n",
    "w1=tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))\n",
    "y=tf.matmul(x,w1)\n",
    "\n",
    "#定义预测多了和预测少了的成本\n",
    "loss_less=10\n",
    "loss_more=1\n",
    "loss=tf.reduce_sum(tf.where(tf.greater(y,y_),\n",
    "                            (y-y_)*loss_more,\n",
    "                            (y_-y)*loss_less))\n",
    "#优化器\n",
    "train_step=tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#通过随机数生成一个模拟数据集\n",
    "rdm=RandomState(1)\n",
    "dataset_size=128\n",
    "X=rdm.rand(dataset_size,2)\n",
    "\n",
    "#设置回归的正确值为两个输入的和加上一个随机量。加上随机量是为了\n",
    "#加入不可预测的噪音，否则不同损失函数的意义就不大了，\n",
    "#因为不同损失函数都会在能完全预测正确的时候最低。\n",
    "#一般来说，噪音为一个均值为0的小量，所以这里噪声设置为-0.05~0.05的随机数\n",
    "Y=[[x1+x2+rdm.rand()/10.0-0.05] for (x1,x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    STEPS=5000\n",
    "    for i in range(STEPS):\n",
    "        start=(i*batch_size)%dataset_size\n",
    "        end=min(start+batch_size,dataset_size)\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是说预测函数是1.02x1+1.04x2，这要比x1+x2大，因为在损失函数中指定预测少了的损失更大。  \n",
    "通过这个样例可以感受到，对于相同的神经网络，不同的损失函数会对训练得到的模型产生重要影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 神经网络优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法：优化单个参数值  \n",
    "反向传播算法：在所有参数上使用梯度下降法 \n",
    "参数更新公式：\n",
    "$$\n",
    "\\theta_{n+1}=\\theta_n-\\eta\\frac{\\partial}{\\partial \\theta_n}J(\\theta_n)\n",
    "$$\n",
    "其中$\\eta$为学习率，$J(\\theta)$为损失函数  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的优化过程可以分为两个阶段：  \n",
    "第一阶段：通过前向传播算法得到预测值，并将预测值和真实值做对比得到两者之间的差距  \n",
    "第二阶段：通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降法更新每一个参数  \n",
    "注意：梯度下降法不能确保被优化的函数达到全局最优解。参数的初始值很大程度影响最后的结果。只有当损失函数为凸函数时，梯度下降法才能保证达到全局最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降算法与随机梯度下降算法的折中——每次计算一小部分训练数据的损失函数。这一小部分数据称之为batch。通过矩阵运算，每次在一个batch上优化神经网络的参数并不会比单个数据慢太多，另一方面，使用一个batch可以大大减少收敛所需要的迭代次数，同时可以使收敛的结果更加接近梯度下降的效果。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出神经网络大致遵循的训练过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#神经网络的一般训练过程\n",
    "batch_size=n\n",
    "\n",
    "#每次读取一小部分数据作为当前的训练数据来执行反向传播算法\n",
    "x=tf.placeholder(tf.float32,shape=(batch_size,2),name=\"x-input\")\n",
    "y_=tf.placeholder(tf.float32,shape=(batch_size,1),name=\"y-input\")\n",
    "\n",
    "#定义神经网络结构和优化算法\n",
    "loss=...\n",
    "train_step=tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#训练神经网络\n",
    "with tf.Session() as sess:\n",
    "     #参数初始化\n",
    "    tf.global_variables_initializer().run()\n",
    "    #迭代更新参数\n",
    "    for i in range(STEPS):\n",
    "        #准备batch_size个训练数据，一般将所有训练数据随机打乱后再选取可以得到\n",
    "        #更好的优化效果\n",
    "        start,end=...\n",
    "        sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
